{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10028340,"sourceType":"datasetVersion","datasetId":6175990}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Task 1: Dataset Setup**\n\nIn this task, we will:\n1. Download and upload the CUB-200-2011 dataset to the Kaggle notebook.\n2. Extract the dataset and explore its folder structure to understand its contents.\n3. Load and display metadata, including:\n   - Class labels (`classes.txt`)\n   - File paths (`images.txt`)\n   - Train/test split (`train_test_split.txt`)","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.utils import plot_model\nfrom transformers import BertTokenizer\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Metadata file paths\nclasses_file = \"/kaggle/input/assignment-3-cub200-2011/CUB_200_2011/classes.txt\"\nimages_file = \"/kaggle/input/assignment-3-cub200-2011/CUB_200_2011/images.txt\"\ntrain_test_split_file = \"/kaggle/input/assignment-3-cub200-2011/CUB_200_2011/train_test_split.txt\"\n\n# Step 1: Display folder structure\nprint(\"Dataset folder structure:\")\nfor root, dirs, files in os.walk(\"/kaggle/input/assignment-3-cub200-2011/\"):\n    print(f\"Root: {root}\")\n    print(f\"Dirs: {dirs}\")\n    print(f\"Files: {files[:5]}\")  # Note: Displaying the first 5 rows of each metadata file as a sample for clarity.\n    print(\"\\n\")\n\n# Step 2: Load metadata into pandas DataFrames\nclasses_df = pd.read_csv(classes_file, sep=\" \", header=None, names=[\"Class_ID\", \"Class_Name\"])\nimages_df = pd.read_csv(images_file, sep=\" \", header=None, names=[\"Image_ID\", \"Image_Path\"])\ntrain_test_df = pd.read_csv(train_test_split_file, sep=\" \", header=None, names=[\"Image_ID\", \"Is_Training\"])\n\n# Step 3: Display samples from each metadata file\nprint(\"Classes Metadata (first 5 rows):\")\nprint(classes_df.head(), \"\\n\")\n\nprint(\"Images Metadata (first 5 rows):\")\nprint(images_df.head(), \"\\n\")\n\nprint(\"Train/Test Split Metadata (first 5 rows):\")\nprint(train_test_df.head(), \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Task 2: Data Preprocessing**\n\nIn this task, we will:\n1. Preprocess the textual descriptions using a BERT tokenizer to create input embeddings for all images.\n2. Resize and normalize all bird images to 64 Ã— 64 pixels.\n3. Create a TensorFlow dataset pipeline that combines the image data and BERT embeddings for the entire dataset.\n\n### Deliverables:\n- Python scripts to preprocess the text and images.\n- Output displaying the shape of batches (images and embeddings).","metadata":{}},{"cell_type":"code","source":"# File paths\nimage_root = \"/kaggle/input/assignment-3-cub200-2011/CUB_200_2011/images\"\n\n# Initialize BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Mock textual descriptions for simplicity (replace with actual descriptions if available)\ntext_descriptions = [f\"Description for image {i+1}\" for i in range(11788)]\n\n# Step 1: Tokenize textual descriptions\ndef tokenize_texts(texts, tokenizer, max_length=128):\n    print(\"Tokenizing textual descriptions...\")\n    tokenized_input_ids = []\n    tokenized_attention_masks = []\n    \n    for text in tqdm(texts, desc=\"Tokenizing text\"):\n        tokens = tokenizer(\n            text,\n            max_length=max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"np\",\n        )\n        tokenized_input_ids.append(tokens['input_ids'][0])\n        tokenized_attention_masks.append(tokens['attention_mask'][0])\n    \n    return np.array(tokenized_input_ids), np.array(tokenized_attention_masks)\n\ninput_ids, attention_masks = tokenize_texts(text_descriptions, tokenizer)\n\n# Step 2: Process images\ndef process_image(image_path):\n    img = Image.open(image_path).convert('RGB')\n    img = img.resize((64, 64))  # Resize to 64x64\n    img_array = np.array(img) / 255.0  # Normalize pixel values to [0, 1]\n    return img_array\n\ndef load_images(image_root):\n    print(\"Processing images...\")\n    image_data = []\n    image_paths = sorted([os.path.join(dp, f) for dp, dn, fn in os.walk(image_root) for f in fn if f.endswith('.jpg')])\n    \n    with tqdm(total=len(image_paths), desc=\"Processing images\") as pbar:\n        for img_path in image_paths:\n            image_data.append(process_image(img_path))\n            pbar.update(1)  # Update progress bar dynamically\n    return np.array(image_data)\n\nimage_data = load_images(image_root)\n\n# Step 3: Combine images and text embeddings into a TensorFlow dataset\ndef create_dataset(image_data, input_ids, attention_masks, batch_size=32):\n    dataset = tf.data.Dataset.from_tensor_slices((image_data, (input_ids, attention_masks)))\n    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n    return dataset\n\nbatch_size = 32\ndataset = create_dataset(image_data, input_ids, attention_masks, batch_size=batch_size)\n\n# Display the shape of batches\nfor images, (ids, masks) in dataset.take(1):\n    print(f\"Image batch shape: {images.shape}\")\n    print(f\"Text embeddings shape: {ids.shape}, Attention masks shape: {masks.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Task 3: GAN Implementation**\n\n### Objectives:\n1. **Implement the Generator Model**:\n   - A neural network conditioned on BERT embeddings, capable of generating bird images of size `(64, 64, 3)`.\n2. **Implement the Discriminator Model**:\n   - A neural network that distinguishes real bird images from fake images produced by the generator.\n3. **Combine the Models into a cGAN**:\n   - Integrate the generator and discriminator into a Conditional GAN (cGAN) architecture.\n4. **Visualize the Architectures**:\n   - Save visualizations of the following model architectures:\n     - Generator\n     - Discriminator\n     - Combined cGAN\n\n### Deliverables:\n- Python scripts defining the GAN, cGAN, and their components.\n- Saved visualizations of the model architectures:\n  - `generator.png`\n  - `discriminator.png`\n  - `cgan.png`\n","metadata":{}},{"cell_type":"code","source":"# Generator Model\ndef build_generator(input_dim, embedding_dim=128, image_size=(64, 64, 3)):\n    input_embedding = layers.Input(shape=(input_dim,), name=\"BERT_Embedding\")\n\n    x = layers.Dense(8 * 8 * 256, activation=\"relu\")(input_embedding)\n    x = layers.Reshape((8, 8, 256))(x)\n\n    x = layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\", activation=\"relu\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding=\"same\", activation=\"relu\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2DTranspose(3, (4, 4), strides=(2, 2), padding=\"same\", activation=\"tanh\")(x)\n\n    generator = Model(input_embedding, x, name=\"Generator\")\n    return generator\n\n# Discriminator Model\ndef build_discriminator(image_size=(64, 64, 3), embedding_dim=128):\n    image_input = layers.Input(shape=image_size, name=\"Image_Input\")\n    embedding_input = layers.Input(shape=(embedding_dim,), name=\"BERT_Embedding\")\n\n    x = layers.Conv2D(64, (4, 4), strides=(2, 2), padding=\"same\", activation=\"leaky_relu\")(image_input)\n    x = layers.Conv2D(128, (4, 4), strides=(2, 2), padding=\"same\", activation=\"leaky_relu\")(x)\n    x = layers.Flatten()(x)\n\n    y = layers.Dense(8 * 8 * 128, activation=\"relu\")(embedding_input)\n    y = layers.Reshape((8, 8, 128))(y)\n    y = layers.Flatten()(y)\n\n    combined = layers.Concatenate()([x, y])\n    combined = layers.Dense(256, activation=\"leaky_relu\")(combined)\n    combined = layers.Dense(1, activation=\"sigmoid\")(combined)\n\n    discriminator = Model([image_input, embedding_input], combined, name=\"Discriminator\")\n    return discriminator\n\n# cGAN Model\ndef build_cgan(generator, discriminator):\n    discriminator.trainable = False\n\n    input_embedding = layers.Input(shape=(128,), name=\"BERT_Embedding\")\n    generated_image = generator(input_embedding)\n    validity = discriminator([generated_image, input_embedding])\n\n    cgan = Model(input_embedding, validity, name=\"cGAN\")\n    return cgan\n\n# Build Models\nembedding_dim = 128\nimage_size = (64, 64, 3)\ngenerator = build_generator(input_dim=embedding_dim, image_size=image_size)\ndiscriminator = build_discriminator(image_size=image_size, embedding_dim=embedding_dim)\ncgan = build_cgan(generator, discriminator)\n\n# Compile Discriminator\noptimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\ndiscriminator.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n\n# Compile cGAN\ncgan.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n\n# Save Model Visualizations\nplot_model(generator, to_file=\"generator.png\", show_shapes=True, show_layer_names=True)\nplot_model(discriminator, to_file=\"discriminator.png\", show_shapes=True, show_layer_names=True)\nplot_model(cgan, to_file=\"cgan.png\", show_shapes=True, show_layer_names=True)\n\n# Summary Outputs\nprint(\"Generator Summary:\")\ngenerator.summary()\nprint(\"Discriminator Summary:\")\ndiscriminator.summary()\nprint(\"cGAN Summary:\")\ncgan.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Task 4: Training the cGAN**\n\nIn this task, we will:\n1. Implement the training loop for the cGAN, which includes:\n   - Updating the generator to improve the quality of generated images.\n   - Updating the discriminator to distinguish between real and fake images.\n2. Track and visualize the loss values for both the generator and the discriminator during training.\n\n### Deliverables:\n- A Python script that implements the cGAN training loop.\n- Loss plots for the generator and discriminator to observe the progression of training.","metadata":{}},{"cell_type":"code","source":"# Constants\nlatent_dim = 128\n\n# Define loss functions\ndef generator_loss(fake_output):\n    return tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n        tf.ones_like(fake_output), fake_output\n    )\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n        tf.ones_like(real_output), real_output\n    )\n    fake_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n        tf.zeros_like(fake_output), fake_output\n    )\n    return real_loss + fake_loss\n\n# Optimizers\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n\n# Training step\n@tf.function\ndef train_step(generator, discriminator, images, bert_embeddings):\n    noise = tf.random.normal([images.shape[0], latent_dim])\n\n    # Ensure discriminator is trainable\n    discriminator.trainable = True\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(bert_embeddings, training=True)\n\n        real_output = discriminator([images, bert_embeddings], training=True)\n        fake_output = discriminator([generated_images, bert_embeddings], training=True)\n\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n\n    return gen_loss, disc_loss\n\n# Training loop\ndef train(generator, discriminator, dataset, epochs):\n    gen_losses = []\n    disc_losses = []\n\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        epoch_gen_loss = 0\n        epoch_disc_loss = 0\n\n        for images, (bert_embeddings, _) in tqdm(dataset, desc=f\"Epoch {epoch + 1}\", leave=False):\n            gen_loss, disc_loss = train_step(generator, discriminator, images, bert_embeddings)\n            epoch_gen_loss += gen_loss\n            epoch_disc_loss += disc_loss\n\n        gen_losses.append(epoch_gen_loss / len(dataset))\n        disc_losses.append(epoch_disc_loss / len(dataset))\n\n        print(f\"Generator Loss: {gen_losses[-1]:.4f}, Discriminator Loss: {disc_losses[-1]:.4f}\")\n\n    return gen_losses, disc_losses\n\n# Plotting function\ndef plot_losses(gen_losses, disc_losses):\n    plt.figure(figsize=(10, 5))\n    plt.plot(gen_losses, label=\"Generator Loss\")\n    plt.plot(disc_losses, label=\"Discriminator Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.title(\"Generator and Discriminator Loss\")\n    plt.show()\n\n# Training execution\nEPOCHS = 50\nprint(\"Starting training...\")\ngen_losses, disc_losses = train(generator, discriminator, dataset, EPOCHS)\n\n# Plot losses\nplot_losses(gen_losses, disc_losses)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Task 5: Evaluation and Reflection**\n\nIn this task, we will:\n1. Generate synthetic bird images using the trained cGAN:\n   - Use the generator to create a batch of images conditioned on BERT embeddings.\n   - Visualize the generated images to assess their quality.\n2. Reflect on the quality of the generated images:\n   - Evaluate the realism and diversity of the synthetic images.\n   - Identify any noticeable artifacts or issues in the generated outputs.\n3. Suggest potential improvements to the model or training process based on the observations.\n\n### Deliverables:\n- A Python script to generate and visualize synthetic bird images.\n- A written reflection (3-5 sentences) discussing the quality of the generated images and proposing potential improvements.","metadata":{}},{"cell_type":"code","source":"# Ensure BERT embeddings are correctly defined\n# Use the embeddings from your preprocessing pipeline (Task 2)\nif 'input_ids' in locals():\n    bert_embeddings = input_ids  # Replace with the correct variable if named differently\nelse:\n    raise ValueError(\"BERT embeddings (input_ids) not found. Ensure Task 2 has been completed.\")\n\n# Function to generate and visualize synthetic bird images\ndef generate_synthetic_images(generator, embeddings, num_images=10):\n    # Ensure the number of images does not exceed available embeddings\n    num_images = min(num_images, len(embeddings))\n\n    # Select embeddings sequentially or randomly (based on your preference)\n    selected_embeddings = embeddings[:num_images]\n\n    # Generate synthetic images\n    print(\"Generating synthetic bird images...\")\n    synthetic_images = generator.predict(selected_embeddings, verbose=0)\n\n    # Rescale images from [-1, 1] to [0, 1] for visualization\n    synthetic_images = (synthetic_images + 1) / 2.0\n\n    # Visualize the generated images\n    plt.figure(figsize=(15, 5))\n    for i in range(num_images):\n        plt.subplot(1, num_images, i + 1)\n        plt.imshow(synthetic_images[i])\n        plt.axis('off')\n    plt.suptitle(\"Generated Synthetic Bird Images\", fontsize=16)\n    plt.show()\n\n# Number of images to generate\nnum_images_to_generate = 10\n\n# Call the function to generate and visualize images\ngenerate_synthetic_images(generator, bert_embeddings, num_images=num_images_to_generate)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Knowledge Questions**\n\n### 1. GANs often face mode collapse, where the generator produces limited variations of data. What techniques can be introduced to mitigate mode collapse, and how would you evaluate their effectiveness in this assignmentâ€™s context?\n\n**Answer**:  \nMode collapse occurs when the generator produces limited variations of outputs. Techniques to mitigate this include:  \n- **Mini-batch discrimination**: Introduce a layer in the discriminator to compare samples within a batch to encourage diversity.  \n- **Feature matching**: Modify the generatorâ€™s objective to match the discriminatorâ€™s intermediate layer features instead of focusing solely on fooling the discriminator.  \n- **Spectral normalization**: Apply this to the discriminator to stabilize training and improve gradient flows.  \n- **Noise regularization**: Add random noise to the input or latent space to encourage variability.  \n\nIn this assignmentâ€™s context, evaluating effectiveness could involve analyzing the diversity of generated bird images using metrics like the FrÃ©chet Inception Distance (FID) or qualitative assessment of variations in key attributes such as color, size, or pose.\n\n---\n\n### 2. Compare and contrast Wasserstein GANs (WGANs) with standard GANs in terms of training stability and convergence. Would WGAN principles be beneficial for the cGAN architecture used here? Why or why not?\n\n**Answer**:  \nWGANs differ from standard GANs in the following ways:\n- **Training Stability**: WGANs use the Earth Mover's Distance (Wasserstein distance) as a loss function, which provides smoother gradients and avoids vanishing gradient issues common in standard GANs.\n- **Convergence**: WGANs tend to converge more reliably due to the use of gradient clipping and critic-based training.  \n\nFor the cGAN in this assignment, WGAN principles could be beneficial as they may stabilize training, especially given the adversarial dynamics between the generator and discriminator. However, integrating WGAN principles would require modifications like replacing the binary cross-entropy loss with Wasserstein loss and potentially rebalancing the discriminator's training frequency.\n\n---\n\n### 3. How would you assess whether BERT embeddings capture enough semantic information relevant to the bird species in the dataset? Propose a method for evaluating their effectiveness.\n\n**Answer**:  \nTo assess the effectiveness of BERT embeddings in capturing semantic information:\n1. **Embedding Visualization**: Use dimensionality reduction techniques like t-SNE or PCA to visualize the embeddings and check for clustering corresponding to bird species.\n2. **Classification Task**: Train a simple classifier (e.g., logistic regression) on the embeddings to predict bird species and evaluate its accuracy.  \n3. **Semantic Correlation**: Calculate the cosine similarity between embeddings of similar species descriptions and compare them with dissimilar species. High intra-species similarity and low inter-species similarity indicate effectiveness.\n\n---\n\n### 4. GAN training is inherently unstable due to the adversarial dynamics between the generator and discriminator. Propose a method to detect instability early in the training process and adjust hyperparameters dynamically to stabilize training.\n\n**Answer**:  \nTo detect instability early:\n- **Monitor Loss Trends**: Plot the generator and discriminator losses. Oscillating or diverging losses indicate instability.\n- **Gradient Norms**: Track gradient norms for exploding or vanishing gradients.  \n- **Diversity Metrics**: Measure the diversity of generated images to detect mode collapse.  \n\nDynamic adjustments:\n- **Learning Rate Adjustment**: Reduce the learning rate if instability is detected.\n- **Training Frequency**: Adjust the training frequency of the generator and discriminator to balance their updates.\n- **Regularization**: Introduce techniques like weight decay or gradient penalty to stabilize gradients.\n\n---\n\n### 5. Discuss the ethical implications of using cGANs in sensitive applications such as healthcare or media generation. What guidelines would you propose for responsible use?\n\n**Answer**:  \nEthical implications of cGANs include:\n- **Misinformation**: cGANs could generate fake content, leading to the spread of misinformation.\n- **Bias Propagation**: Training on biased datasets may lead to biased outputs.  \n- **Privacy Concerns**: Generated data resembling real individuals or entities could breach privacy.  \n\nGuidelines for responsible use:\n- **Dataset Transparency**: Ensure datasets are unbiased and sourced ethically.  \n- **Content Labeling**: Clearly label synthetic content to prevent misuse.  \n- **Restricted Access**: Use cGANs in controlled environments and for positive applications like medical imaging or artistic creativity.\n\n---\n\n### 6. Text-to-image generation tasks raise concerns about intellectual property rights, especially when trained on publicly available datasets. Analyze how such concerns apply to this assignment and suggest ways to mitigate potential legal risks.\n\n**Answer**:  \nConcerns:\n- **Copyright Issues**: Training on datasets with copyrighted images could infringe intellectual property rights.  \n- **Derivative Works**: Generated images may resemble original copyrighted works, raising legal issues.\n\nMitigation:\n- **Dataset Licensing**: Use datasets with clear licensing terms allowing derivative works.  \n- **Attribution**: Acknowledge the sources of datasets.  \n- **Content Screening**: Regularly audit generated images to ensure they do not closely replicate copyrighted material.\n\n---\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}